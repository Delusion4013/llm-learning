{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a48844",
   "metadata": {},
   "source": [
    "# Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b23df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# PDF processing\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "\n",
    "# Vector database and embeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LLM integration (using OpenAI as example)\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "# Text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings for cleaner output\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97833d1",
   "metadata": {},
   "source": [
    "# PDF Parsing & Text Extraction\n",
    "\n",
    "In this section, I implemented a `Parser` class for PDF financial reports, extracting raw text, metadata and tables from the input PDF document for later use.\n",
    "\n",
    "I used [Nvidia's 2024 annual report from StockLight](https://stocklight.com/stocks/us/nasdaq-nvda/nvidia/annual-reports/nasdaq-nvda-2024-10K-24660316.pdf) for demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0c2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialPDFParser:\n",
    "    \"\"\"Parse and extract structured information from financial PDF statements\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract text and metadata from PDF file\"\"\"\n",
    "        \n",
    "        # Define structure for extracted data\n",
    "        extracted_data = {\n",
    "            'raw_text': '',\n",
    "            'pages': [],\n",
    "            'tables': [],\n",
    "            'metadata': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                # Extract metadata\n",
    "                extracted_data['metadata'] = {\n",
    "                    'total_pages': len(pdf.pages),\n",
    "                    'title': getattr(pdf.metadata, 'title', 'Unknown'),\n",
    "                    'author': getattr(pdf.metadata, 'author', 'Unknown'),\n",
    "                    'creation_date': getattr(pdf.metadata, 'creation_date', None)\n",
    "                }\n",
    "                \n",
    "                # Extract text and tables from each page\n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        extracted_data['pages'].append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'text': page_text\n",
    "                        })\n",
    "                        extracted_data['raw_text'] += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\\n\"\n",
    "                    \n",
    "                    # Extract tables\n",
    "                    tables = page.extract_tables()\n",
    "                    if tables:\n",
    "                        for table_idx, table in enumerate(tables):\n",
    "                            extracted_data['tables'].append({\n",
    "                                'page': page_num + 1,\n",
    "                                'table_index': table_idx,\n",
    "                                'data': table\n",
    "                            })            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing PDF: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"PDF parsed successfully!\")\n",
    "        print(f\"Pages: {extracted_data['metadata']['total_pages']}\")\n",
    "        print(f\"Tables found: {len(extracted_data['tables'])}\")\n",
    "        print(f\"Total text length: {len(extracted_data['raw_text'])} characters\")\n",
    "        \n",
    "        return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55231b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF parsed successfully!\n",
      "Pages: 89\n",
      "Tables found: 56\n",
      "Total text length: 341204 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize parser and create sample data\n",
    "pdf_parser = FinancialPDFParser()\n",
    "# Get results\n",
    "pdf_data = pdf_parser.extract_text_from_pdf(\"nvda-2024.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2bfb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Document metadata: {'total_pages': 89, 'title': 'Unknown', 'author': 'Unknown', 'creation_date': None}\n",
      "============================================================\n",
      "Text preview: \n",
      "--- Page 1 ---\n",
      "stocklight.com > Stocks > United States\n",
      "NVIDIA Corporation > Annual Reports > 2024\n",
      "Annual Report\n",
      "NVIDIA Corporation Annual Report 2024\n",
      "Form 10-K (NASDAQ:NVDA)\n",
      "Published: February 21st,...\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"=\"*60)\n",
    "print(f\"Document metadata: {pdf_data['metadata']}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Text preview: {pdf_data['raw_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ead2a",
   "metadata": {},
   "source": [
    "# Vector Database Setup\n",
    "\n",
    "Build the vector database with given pdf data with `FinancialRAGDatabase` class. General pipeline: \n",
    "1. Use `raw_text` from previous parser\n",
    "2. Split raw_text into different section \n",
    "3. Overlap chunking for each section\n",
    "4. Encode chunks with selected `embedding_model`\n",
    "5. Add embedding vectors to vector database\n",
    "\n",
    "`FinancialRAGDatabase` also includes a wrapper `search` function to define how to perform search in this db.\n",
    "\n",
    "For demostration purpose, only one pdf file included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959277ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialRAGDatabase:\n",
    "    \"\"\"Vector database for financial document RAG\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model = None, collection_name=\"financial_statements\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_model = SentenceTransformer('all-mpnet-base-v2') if embedding_model == None else embedding_model\n",
    "        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "        self.collection = self._setup_collection()\n",
    "\n",
    "        # print(f\"Embedding model: {self.embedding_model}\")\n",
    "        \n",
    "    def _setup_collection(self):\n",
    "        \"\"\"Initialize or get existing ChromaDB collection\"\"\"\n",
    "        try:\n",
    "            # Try to get existing collection\n",
    "            collection = self.chroma_client.get_collection(self.collection_name)\n",
    "            print(f\"Using existing collection: {self.collection_name}\")\n",
    "        except:\n",
    "            # Create new collection\n",
    "            collection = self.chroma_client.create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"Financial statement chunks for RAG\"}\n",
    "            )\n",
    "            print(f\"Created new collection: {self.collection_name}\")\n",
    "        \n",
    "        return collection\n",
    "    \n",
    "\n",
    "    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "        \"\"\"Split financial text into overlapping chunks\"\"\"\n",
    "        \n",
    "        # Split into sentences first\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            # If adding this sentence would exceed chunk size, save current chunk\n",
    "            if current_length + sentence_length > chunk_size and current_chunk:\n",
    "                chunks.append({\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'length': current_length,\n",
    "                    'sentence_count': len(sent_tokenize(current_chunk))\n",
    "                })\n",
    "                \n",
    "                # Start new chunk with one overlap sentence.\n",
    "                overlap_sentences = sent_tokenize(current_chunk)[-2:] if len(sent_tokenize(current_chunk)) >= 2 else []\n",
    "                current_chunk = \" \".join(overlap_sentences) + \" \" + sentence\n",
    "                current_length = len(current_chunk)\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence\n",
    "                current_length += sentence_length\n",
    "        \n",
    "        # Add the final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'length': current_length,\n",
    "                'sentence_count': len(sent_tokenize(current_chunk))\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def extract_financial_sections(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Split financial statements into different sections from the text\"\"\"\n",
    "        \n",
    "        sections = {}\n",
    "        \n",
    "        # Define section patterns via regex\n",
    "        section_patterns = {\n",
    "            'income_statement': r'(CONSOLIDATED STATEMENTS OF OPERATIONS|INCOME)(.*?)(?=CONSOLIDATED BALANCE SHEETS|BALANCE SHEET|$)',\n",
    "            'balance_sheet': r'(CONSOLIDATED BALANCE SHEETS|BALANCE SHEET)(.*?)(?=MANAGEMENT\\'S DISCUSSION|CASH FLOWS|$)',\n",
    "            'management_discussion': r'(MANAGEMENT\\'S DISCUSSION AND ANALYSIS|MD&A)(.*?)(?=RISK FACTORS|NOTES TO|$)',\n",
    "            'risk_factors': r'(RISK FACTORS)(.*?)(?=NOTES TO|LEGAL|$)'\n",
    "        }\n",
    "        \n",
    "        for section_name, pattern in section_patterns.items():\n",
    "            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                sections[section_name] = match.group(2).strip()\n",
    "        \n",
    "        # If no sections found, treat entire text as general content\n",
    "        if not sections:\n",
    "            sections['general_content'] = text\n",
    "            \n",
    "        print(f\"Extracted {len(sections)} financial sections: {list(sections.keys())}\")\n",
    "        return sections\n",
    "    \n",
    "    def add_document(self, pdf_data: Dict, document_id: str = \"financial_statement_1\", chunk_size: int = 500):\n",
    "        \"\"\"Main funciton to add financial document to vector database\"\"\"\n",
    "        \n",
    "        text = pdf_data['raw_text']\n",
    "        metadata = pdf_data['metadata']\n",
    "        \n",
    "        # Extract financial sections\n",
    "        sections = self.extract_financial_sections(text)\n",
    "        \n",
    "        # Create chunks for each section\n",
    "        all_chunks = []\n",
    "        all_metadatas = []\n",
    "        all_ids = []\n",
    "        \n",
    "        for section_name, section_text in sections.items():\n",
    "            chunks = self.chunk_text(section_text, chunk_size)\n",
    "            print(f\"Created {len(chunks)} text chunks in {section_name}\")\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{document_id}_{section_name}_{i}\"\n",
    "                \n",
    "                chunk_metadata = {\n",
    "                    'document_id': document_id,\n",
    "                    'section': section_name,\n",
    "                    'chunk_index': i,\n",
    "                    'chunk_length': chunk['length'],\n",
    "                    'sentence_count': chunk['sentence_count'],\n",
    "                    'document_title': metadata.get('title', 'Unknown'),\n",
    "                    'total_pages': metadata.get('total_pages', 0)\n",
    "                }\n",
    "                \n",
    "                all_chunks.append(chunk['text'])\n",
    "                all_metadatas.append(chunk_metadata)\n",
    "                all_ids.append(chunk_id)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(\"Generating embeddings...\")\n",
    "        embeddings = self.embedding_model.encode(all_chunks).tolist()\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=all_chunks,\n",
    "            metadatas=all_metadatas,\n",
    "            ids=all_ids\n",
    "        )\n",
    "        \n",
    "        print(f\"Added {len(all_chunks)} chunks to vector database\")\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(all_chunks),\n",
    "            'sections': list(sections.keys()),\n",
    "            'document_id': document_id\n",
    "        }\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 5, section_filter: str = None) -> Dict:\n",
    "        \"\"\"A wrapper function to perform search in the vector database\"\"\"\n",
    "        \n",
    "        # Build where clause for filtering\n",
    "        where_clause = {}\n",
    "        if section_filter:\n",
    "            where_clause['section'] = section_filter\n",
    "        \n",
    "        # Perform semantic search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[self.embedding_model.encode(query).tolist()],\n",
    "            n_results=n_results,\n",
    "            where=where_clause if where_clause else None,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'results': results,\n",
    "            'num_results': len(results['documents'][0]) if results['documents'] else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a1fd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: financial_statements\n",
      "Extracted 4 financial sections: ['income_statement', 'balance_sheet', 'management_discussion', 'risk_factors']\n",
      "Created 274 text chunks in income_statement\n",
      "Created 51 text chunks in balance_sheet\n",
      "Created 1 text chunks in management_discussion\n",
      "Created 1 text chunks in risk_factors\n",
      "Generating embeddings...\n",
      "Added 327 chunks to vector database\n",
      "============================================================\n",
      "Indexing Summary:\n",
      "   Total chunks created: 327\n",
      "   Sections indexed: income_statement, balance_sheet, management_discussion, risk_factors\n"
     ]
    }
   ],
   "source": [
    "# Initialize database and add financial document\n",
    "rag_db = FinancialRAGDatabase()\n",
    "indexing_result = rag_db.add_document(pdf_data)\n",
    "\n",
    "print(f\"=\"*60)\n",
    "print(f\"Indexing Summary:\")\n",
    "print(f\"   Total chunks created: {indexing_result['total_chunks']}\")\n",
    "print(f\"   Sections indexed: {', '.join(indexing_result['sections'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465ce729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Test Search Results for: 'What was the total revenue for NVDA in 2024?'\n",
      "\n",
      "   Result 1 (Similarity: 0.378):\n",
      "   Document ID: financial_statement_1\n",
      "   Chunk Index: 130\n",
      "   Text: Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago. Data Center revenue for fiscal year 2024 was up 217%. Strong demand was driven by enterprise software and consumer internet app...\n",
      "============================================================\n",
      "\n",
      "   Result 2 (Similarity: 0.266):\n",
      "   Document ID: financial_statement_1\n",
      "   Chunk Index: 146\n",
      "   Text: Professional Visualization revenue for fiscal year 2024 was $1.6 billion, up 1% from fiscal year 2023. In Professional Visualization, we announced new GPUs based\n",
      "on the NVIDIA RTX Ada Lovelace archite...\n",
      "============================================================\n",
      "\n",
      "   Result 3 (Similarity: 0.261):\n",
      "   Document ID: financial_statement_1\n",
      "   Chunk Index: 131\n",
      "   Text: Customers across industry verticals access NVIDIA AI infrastructure both through the cloud and on-\n",
      "premises. Data Center compute revenue was up 244% in the fiscal year. Networking revenue was up 133% ...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test search functionality\n",
    "test_query = \"What was the total revenue for NVDA in 2024?\"\n",
    "search_results = rag_db.search(test_query, n_results=3)\n",
    "\n",
    "print(f\"\\n🔍 Test Search Results for: '{test_query}'\")\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    search_results['results']['documents'][0],\n",
    "    search_results['results']['metadatas'][0],\n",
    "    search_results['results']['distances'][0]\n",
    ")):\n",
    "    print(f\"\\n   Result {i+1} (Similarity: {1-distance:.3f}):\")\n",
    "    print(f\"   Document ID: {metadata['document_id']}\")\n",
    "    print(f\"   Chunk Index: {metadata['chunk_index']}\")\n",
    "    print(f\"   Text: {doc[:200]}...\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a380e",
   "metadata": {},
   "source": [
    "We could find that the search successfully located the correct result at page 40 of the original statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c13cf",
   "metadata": {},
   "source": [
    "## Unit test for `chunk_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fac6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_chunk_overlap (__main__.TestChunkText.test_chunk_overlap)\n",
      "Test that chunks have proper overlap ... ok\n",
      "test_chunk_size_limit (__main__.TestChunkText.test_chunk_size_limit)\n",
      "Test that chunks respect size limits ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: test5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_different_chunk_sizes (__main__.TestChunkText.test_different_chunk_sizes)\n",
      "Test chunking with different size parameters ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: test2\n",
      "Created new collection: test7-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_empty_text (__main__.TestChunkText.test_empty_text)\n",
      "Test handling of empty input ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: test7-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_simple_chunking (__main__.TestChunkText.test_simple_chunking)\n",
      "Test basic chunking functionality ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: test3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_single_sentence (__main__.TestChunkText.test_single_sentence)\n",
      "Test chunking of single sentence ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: test1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_very_long_sentence (__main__.TestChunkText.test_very_long_sentence)\n",
      "Test handling of sentences longer than chunk size ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: test4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 6.838s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: test6\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestChunkText(unittest.TestCase):\n",
    "    \"\"\"Unit tests for the chunk_text method\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test fixtures\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_simple_chunking(self):\n",
    "        \"\"\"Test basic chunking functionality\"\"\"\n",
    "        rag_db = FinancialRAGDatabase(collection_name=\"test1\")\n",
    "        text = \"This is sentence one. This is sentence two. This is sentence three.\"\n",
    "        chunks = rag_db.chunk_text(text, chunk_size=50, overlap=10)\n",
    "        \n",
    "        # Should create multiple chunks due to size limit\n",
    "        self.assertGreater(len(chunks), 1)\n",
    "        \n",
    "        # Each chunk should have required fields\n",
    "        for chunk in chunks:\n",
    "            self.assertIn('text', chunk)\n",
    "            self.assertIn('length', chunk)\n",
    "            self.assertIn('sentence_count', chunk)\n",
    "            self.assertIsInstance(chunk['text'], str)\n",
    "            self.assertIsInstance(chunk['length'], int)\n",
    "            self.assertIsInstance(chunk['sentence_count'], int)\n",
    "    \n",
    "    def test_chunk_size_limit(self):\n",
    "        \"\"\"Test that chunks respect size limits\"\"\"\n",
    "        rag_db = FinancialRAGDatabase(collection_name=\"test2\")\n",
    "        text = \"Short sentence. \" * 50  # Create long text\n",
    "        chunk_size = 100\n",
    "        chunks = rag_db.chunk_text(text, chunk_size=chunk_size)\n",
    "        \n",
    "        # Most chunks should be under the size limit (allowing some flexibility for overlap)\n",
    "        oversized_chunks = [c for c in chunks if c['length'] > chunk_size * 1.5]\n",
    "        self.assertLess(len(oversized_chunks), len(chunks) * 0.3)  # Less than 30% oversized\n",
    "    \n",
    "    def test_empty_text(self):\n",
    "        \"\"\"Test handling of empty input\"\"\"\n",
    "        rag_db = FinancialRAGDatabase(collection_name=\"test3\")\n",
    "        chunks = rag_db.chunk_text(\"\", chunk_size=500)\n",
    "        self.assertEqual(len(chunks), 0)\n",
    "    \n",
    "    def test_single_sentence(self):\n",
    "        \"\"\"Test chunking of single sentence\"\"\"\n",
    "        rag_db = FinancialRAGDatabase(collection_name=\"test4\")\n",
    "        text = \"This is a single sentence.\"\n",
    "        chunks = rag_db.chunk_text(text, chunk_size=500)\n",
    "        \n",
    "        self.assertEqual(len(chunks), 1)\n",
    "        self.assertEqual(chunks[0]['text'].strip(), text.strip())\n",
    "        self.assertEqual(chunks[0]['sentence_count'], 1)\n",
    "    \n",
    "    def test_chunk_overlap(self):\n",
    "        \"\"\"Test that chunks have proper overlap\"\"\"\n",
    "        rag_db = FinancialRAGDatabase(collection_name=\"test5\")\n",
    "        text = \"First sentence here. Second sentence follows. Third sentence appears. Fourth sentence concludes.\"\n",
    "        chunks = rag_db.chunk_text(text, chunk_size=40, overlap=10)\n",
    "        \n",
    "        if len(chunks) > 1:\n",
    "            # Check that there's some overlap between consecutive chunks\n",
    "            first_chunk_words = set(chunks[0]['text'].split())\n",
    "            second_chunk_words = set(chunks[1]['text'].split())\n",
    "            overlap_words = first_chunk_words.intersection(second_chunk_words)\n",
    "            self.assertGreater(len(overlap_words), 0, \"Chunks should have overlapping words\")\n",
    "    \n",
    "    def test_very_long_sentence(self):\n",
    "        \"\"\"Test handling of sentences longer than chunk size\"\"\"\n",
    "        rag_db = FinancialRAGDatabase(collection_name=\"test6\")\n",
    "        long_sentence = \"This is a very long sentence that exceeds the chunk size limit. \" * 10\n",
    "        chunks = rag_db.chunk_text(long_sentence, chunk_size=50)\n",
    "        \n",
    "        # Should still create at least one chunk\n",
    "        self.assertGreaterEqual(len(chunks), 1)\n",
    "        # The chunk should contain the long sentence (even if it exceeds limit)\n",
    "        self.assertIn(\"very long sentence\", chunks[0]['text'])\n",
    "    \n",
    "    def test_different_chunk_sizes(self):\n",
    "        \"\"\"Test chunking with different size parameters\"\"\"\n",
    "        rag_db1 = FinancialRAGDatabase(collection_name=\"test7-1\")\n",
    "        rag_db2 = FinancialRAGDatabase(collection_name=\"test7-2\")\n",
    "        text = \"One. Two. Three. Four. Five. Six. Seven. Eight.\"\n",
    "        \n",
    "        # Test small chunks\n",
    "        small_chunks = rag_db1.chunk_text(text, chunk_size=10)\n",
    "        \n",
    "        # Test large chunks\n",
    "        large_chunks = rag_db2.chunk_text(text, chunk_size=200)\n",
    "        \n",
    "        # Small chunk size should create more chunks\n",
    "        self.assertGreaterEqual(len(small_chunks), len(large_chunks))\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"Run the unit tests\"\"\"\n",
    "    unittest.main(argv=[''], exit=False, verbosity=2)\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4772a8a",
   "metadata": {},
   "source": [
    "# RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8642fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialRAGSystem:\n",
    "    \"\"\"Complete RAG system for financial Q&A\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_database: FinancialRAGDatabase):\n",
    "        self.rag_db = rag_database\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def prepare_context(self, search_results: Dict, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Prepare context from search results within token limit\"\"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for doc, metadata in zip(\n",
    "            search_results['results']['documents'][0],\n",
    "            search_results['results']['metadatas'][0]\n",
    "        ):\n",
    "            # Format the context chunk\n",
    "            chunk_text = f\"[{metadata['section'].upper()}]: {doc}\"\n",
    "            chunk_tokens = self.count_tokens(chunk_text)\n",
    "            \n",
    "            if total_tokens + chunk_tokens <= max_tokens:\n",
    "                context_parts.append(chunk_text)\n",
    "                total_tokens += chunk_tokens\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_answer_openai(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate answer using OpenAI API\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"You are a financial analyst AI assistant. Answer questions about financial statements accurately and concisely based on the provided context. \n",
    "\n",
    "Guidelines:\n",
    "1. Use only information from the provided context\n",
    "2. Include specific numbers and percentages when available\n",
    "3. If the context doesn't contain enough information, say so clearly\n",
    "4. Provide clear, professional financial analysis\n",
    "5. Format financial numbers properly (e.g., $125.4 million, 7.1% growth)\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Context from financial statements:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a concise answer based on the financial data above.\"\"\"\n",
    "\n",
    "        client = OpenAI()\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def generate_answer_local(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate answer using local processing (fallback when no OpenAI API)\"\"\"\n",
    "        \n",
    "        # Simple keyword-based answer generation for demo\n",
    "        context_lower = context.lower()\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Extract relevant sentences\n",
    "        sentences = sent_tokenize(context)\n",
    "        relevant_sentences = []\n",
    "        \n",
    "        # Simple keyword matching\n",
    "        question_keywords = set(word_tokenize(question_lower)) - set(stopwords.words('english'))\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_words = set(word_tokenize(sentence.lower()))\n",
    "            if question_keywords.intersection(sentence_words):\n",
    "                relevant_sentences.append(sentence)\n",
    "        \n",
    "        if relevant_sentences:\n",
    "            return f\"Based on the financial statements: {' '.join(relevant_sentences[:3])}\"\n",
    "        else:\n",
    "            return \"I couldn't find specific information to answer your question in the provided financial statements.\"\n",
    "    \n",
    "    def answer_question(self, question: str, use_openai: bool = False, section_filter: str = None) -> Dict:\n",
    "        \"\"\"Complete RAG pipeline to answer financial questions\"\"\"\n",
    "        \n",
    "        # Step 1: Search for relevant context\n",
    "        search_results = self.rag_db.search(question, n_results=3, section_filter=section_filter)\n",
    "        \n",
    "        if search_results['num_results'] == 0:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': \"No relevant information found in the financial statements.\",\n",
    "                'context_used': \"\",\n",
    "                'sources': [],\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "        \n",
    "        # Step 2: Prepare context\n",
    "        context = self.prepare_context(search_results)\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        if use_openai:\n",
    "            answer = self.generate_answer_openai(question, context)\n",
    "        else:\n",
    "            answer = self.generate_answer_local(question, context)\n",
    "        \n",
    "        # Step 4: Extract sources\n",
    "        sources = []\n",
    "        for metadata in search_results['results']['metadatas'][0]:\n",
    "            sources.append({\n",
    "                'section': metadata['section'],\n",
    "                'chunk_index': metadata['chunk_index'],\n",
    "                'document_title': metadata['document_title']\n",
    "            })\n",
    "        \n",
    "        # Step 5: Calculate confidence (based on search similarity)\n",
    "        avg_distance = np.mean(search_results['results']['distances'][0])\n",
    "        confidence = max(0.0, 1.0 - avg_distance)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'context_used': context,\n",
    "            'sources': sources,\n",
    "            'confidence': confidence,\n",
    "            'num_sources': search_results['num_results']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc88d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial RAG System initialized successfully!\n",
      "💬 FINANCIAL Q&A DEMONSTRATION\n",
      "==================================================\n",
      "\n",
      "❓ Question 1: What was the total revenue for 2023?\n",
      "🤖 Answer: The total revenue for fiscal year 2023 was $26.974 billion.\n",
      "📊 Confidence: 0.27\n",
      "📚 Sources: 3 chunks from 1 sections\n",
      "🎯 Primary source: income_statement\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question 2: How much did revenue grow from 2023 to 2024?\n",
      "🤖 Answer: Revenue grew from $26,974 million in 2023 to $60,922 million in 2024, representing an increase of $33,948 million, or 126%.\n",
      "📊 Confidence: 0.33\n",
      "📚 Sources: 3 chunks from 1 sections\n",
      "🎯 Primary source: income_statement\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question 3: What are the main risk factors mentioned?\n",
      "🤖 Answer: The main risk factors mentioned include:\n",
      "\n",
      "1. Macroeconomic factors such as inflation, increased interest rates, capital market volatility, global supply chain constraints, and global economic and geopolitical developments.\n",
      "2. Regulatory and legal risks.\n",
      "3. Cybersecurity-related risks, which are overseen by the Company’s Board of Directors. \n",
      "\n",
      "These factors may impact the company's results of operations and demand for its products.\n",
      "📊 Confidence: 0.00\n",
      "📚 Sources: 3 chunks from 1 sections\n",
      "🎯 Primary source: income_statement\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question 4: What are the main drivers of revenue growth?\n",
      "🤖 Answer: The main drivers of revenue growth include:\n",
      "\n",
      "1. **Compute Segment**: A significant increase of 266% due to higher shipments of the NVIDIA Hopper GPU computing platform for training and inference in LLMs, recommendation engines, and generative AI applications.\n",
      "2. **Networking Segment**: A 133% increase driven by higher shipments of InfiniBand.\n",
      "3. **Graphics Revenue**: A 15% year-on-year increase primarily from growth in Gaming, attributed to higher sell-in to partners following the normalization of channel inventory levels.\n",
      "4. **Automotive Revenue**: A 21% increase reflecting growth in self-driving platforms.\n",
      "5. **Professional Visualization**: A modest increase of 1%.\n",
      "\n",
      "Overall, the growth is supported by higher demand and improved sell-in to partners across various segments.\n",
      "📊 Confidence: 0.07\n",
      "📚 Sources: 3 chunks from 1 sections\n",
      "🎯 Primary source: income_statement\n",
      "--------------------------------------------------\n",
      "\n",
      "📈 Q&A SESSION SUMMARY:\n",
      "   Average confidence: 0.17\n",
      "   Average answer length: 350 characters\n",
      "   Average sources used: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG system\n",
    "rag_system = FinancialRAGSystem(rag_db)\n",
    "\n",
    "print(\"Financial RAG System initialized successfully!\")\n",
    "# Demo questions for the financial statement\n",
    "demo_questions = [\n",
    "    \"What was the total revenue for 2023?\",\n",
    "    \"How much did revenue grow from 2023 to 2024?\",\n",
    "    \"What are the main risk factors mentioned?\",\n",
    "    \"What are the main drivers of revenue growth?\",\n",
    "]\n",
    "\n",
    "print(\"💬 FINANCIAL Q&A DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for i, question in enumerate(demo_questions, 1):\n",
    "    print(f\"\\n❓ Question {i}: {question}\")\n",
    "    \n",
    "    # Get answer using local processing\n",
    "    result = rag_system.answer_question(question, use_openai=True)\n",
    "    \n",
    "    print(f\"🤖 Answer: {result['answer']}\")\n",
    "    print(f\"📊 Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"📚 Sources: {result['num_sources']} chunks from {len(set(s['section'] for s in result['sources']))} sections\")\n",
    "    \n",
    "    # Show top source section\n",
    "    if result['sources']:\n",
    "        top_section = result['sources'][0]['section']\n",
    "        print(f\"🎯 Primary source: {top_section}\")\n",
    "    \n",
    "    results_summary.append({\n",
    "        'question': question,\n",
    "        'answer_length': len(result['answer']),\n",
    "        'confidence': result['confidence'],\n",
    "        'num_sources': result['num_sources']\n",
    "    })\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Summary statistics\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "print(f\"\\n📈 Q&A SESSION SUMMARY:\")\n",
    "print(f\"   Average confidence: {summary_df['confidence'].mean():.2f}\")\n",
    "print(f\"   Average answer length: {summary_df['answer_length'].mean():.0f} characters\")\n",
    "print(f\"   Average sources used: {summary_df['num_sources'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66a2c3",
   "metadata": {},
   "source": [
    "# Summary and improvements\n",
    "\n",
    "From the Q&A demo section, we could find the RAG system provides reasonable answers to simple questions (Q1,2) within the income statements sections and basic understanding questions (Q4). \n",
    "\n",
    "As for risk factors analysis (3), it's not targeted at the correct section (we have `risk factors` sections defined), probablly due to the poor section extraction by regex (as we only get 1 text chunk in `risk factors` section), which can be improved later on.\n",
    "\n",
    "Another extension to implement is using the extracted tables from the original PDF data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd47871",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
