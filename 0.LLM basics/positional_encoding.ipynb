{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cf7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaMA's RoPE implementation\n",
    "import torch\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6ec056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "q: torch.Size([4, 2, 4, 4]), k: torch.Size([4, 2, 4, 4])\n",
      "cos: torch.Size([1, 4, 1, 4]), sin: torch.Size([1, 4, 1, 4])\n",
      "\n",
      "After RoPE:\n",
      "q_rope: torch.Size([4, 2, 4, 4]), k_rope: torch.Size([4, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Example usage\n",
    "def create_rope_embeddings(seq_len, dim, base=10000):\n",
    "    \"\"\"Create RoPE cos/sin embeddings\"\"\"\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(seq_len).type_as(inv_freq)\n",
    "    freqs = torch.einsum(\"i,j->ij\", t, inv_freq)  # [seq_len, dim//2]\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)  # [seq_len, dim]\n",
    "    cos = emb.cos()[None, :, None, :]  # [1, seq_len, 1, dim]\n",
    "    sin = emb.sin()[None, :, None, :]  # [1, seq_len, 1, dim]\n",
    "    return cos, sin\n",
    "# Demo\n",
    "batch_size, num_heads, seq_len, head_dim = 4, 2, 4, 4\n",
    "\n",
    "# Create query and key tensors\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "# Create RoPE embeddings\n",
    "cos, sin = create_rope_embeddings(seq_len, head_dim)\n",
    "position_ids = torch.arange(seq_len)\n",
    "\n",
    "print(\"Original shapes:\")\n",
    "print(f\"q: {q.shape}, k: {k.shape}\")\n",
    "print(f\"cos: {cos.shape}, sin: {sin.shape}\")\n",
    "\n",
    "# Apply RoPE\n",
    "q_rope, k_rope = apply_rotary_pos_emb(q, k, cos, sin, position_ids)\n",
    "\n",
    "print(f\"\\nAfter RoPE:\")\n",
    "print(f\"q_rope: {q_rope.shape}, k_rope: {k_rope.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922531f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
