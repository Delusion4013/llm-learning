{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47419c0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook aims to walk through the calculation process of memory & compute (in FLOPs). Some motivating questions:\n",
    "\n",
    "- How long would it take to train a 70B parameter model on 15T tokens on 1024 H100s?\n",
    "- What's the largest model that can you can train on 8 H100s using AdamW (naively)?\n",
    "\n",
    "\n",
    "This notebook is largely inspired by [CS336 Lecture 2 - resource accounting](https://www.youtube.com/watch?v=msHyYioAyNE&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index32323232=2&ab_channel=StanfordOnline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5418a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e7f64",
   "metadata": {},
   "source": [
    "## Memory Accounting\n",
    "\n",
    "### Tensor memory\n",
    "\n",
    "Almost everything (parameters, gradients, activations, optimizer states) are stored as floating point numbers.\n",
    "\n",
    "Different data types a tensor could take:\n",
    "- `float32` - 32bits, 1 for sign, 8 for exponent, 23 for fraction - [wiki](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)\n",
    "    - Also known as `fp32`, `single precision` is the default\n",
    "- `float16` - 16bits, 1 for sign, 5 for exponent, 10 for fraction\n",
    "    - Also known as `fp16`, `half precision`\n",
    "    - Suffers from low dynamic range (especially for small numbers), causing instability in training\n",
    "- `bfloat16` - 16bits, 1 for sign, 8 for exponent, 7 for fraction\n",
    "    - Care more about dynamic range than fraction(same dynamic range as `float32`), proposed by Google Brain (brain floating point)\n",
    "- `fp8` - introduced by NVIDIA - [doc](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)\n",
    "    - Two variation - E4M3 & E5M2\n",
    "\n",
    "\n",
    "Implications on training:\n",
    "- Training with `float32` works, but requires lots of memory\n",
    "- Training with `fp8`, `float16`, and even `bfloat16` is risky as it's instable.\n",
    "- Solution - **mixed precision training** (use lower preicision when possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ecb3071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\n",
      "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n",
      "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n"
     ]
    }
   ],
   "source": [
    "def get_memory_usage(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the memory usage of a PyTorch tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The tensor to calculate memory usage for.\n",
    "        \n",
    "    Returns:\n",
    "        int: Memory usage in bytes.\n",
    "    \"\"\"\n",
    "    return tensor.numel() * tensor.element_size()\n",
    "\n",
    "\n",
    "# Memory usage of fp32 tensors\n",
    "x32 = torch.zeros(4, 8) \n",
    "assert x32.dtype == torch.float32 \n",
    "assert x32.numel() == 4 * 8\n",
    "assert x32.element_size() == 4  # Float is 4 bytes\n",
    "assert get_memory_usage(x32) == 4 * 8 * 4  # 128 bytes\n",
    "\n",
    "# Memory usage of fp16 tensors\n",
    "x16 = torch.zeros(4, 8, dtype=torch.float16)\n",
    "assert x16.element_size() == 2  # Half is 2 bytes\n",
    "assert get_memory_usage(x16) == 4 * 8 * 2 \n",
    "\n",
    "x16 = torch.tensor([1e-8], dtype=torch.float16)\n",
    "assert x16 == 0 # Underflow to zero\n",
    "\n",
    "xb16 = torch.tensor([1e-8], dtype=torch.bfloat16)\n",
    "assert xb16 == 1e-8 # No underflow, bfloat16 has larger dynamic range\n",
    "\n",
    "print(torch.finfo(torch.float32))  # Float32 info\n",
    "print(torch.finfo(torch.float16))  # Float16 info\n",
    "print(torch.finfo(torch.bfloat16))  # BFloat16 info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d8667",
   "metadata": {},
   "source": [
    "### Example for GPT2 parameter verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794ca343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f5aacfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in GPT-2: 124.438272M parameters\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "# GPT-2 model configuration\n",
    "\n",
    "hidden_size = 768\n",
    "vocab_size = 50257\n",
    "\n",
    "# input embedding - weight sharing with output embedding\n",
    "wte_size = vocab_size * hidden_size\n",
    "# position embedding\n",
    "wpe_size = 1024 * hidden_size  # 1024 is the max sequence length\n",
    "\n",
    "# transformer block\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "head_dim = 64\n",
    "\n",
    "layer_norm = 2 * hidden_size # LayerNorm has two parameters: weight and bias\n",
    "attn = 3 * hidden_size * hidden_size + 3 * hidden_size  # Q, K, V weights + biases\n",
    "attn_proj = hidden_size * hidden_size + hidden_size  # Output linear layer weights + biases\n",
    "first_fp = 4 * hidden_size * hidden_size + 4 * hidden_size  # first FC layer weights + biases\n",
    "second_fp = hidden_size * hidden_size * 4 + hidden_size  #  second FC layer weights + biases\n",
    "\n",
    "transformer = num_layers * (\n",
    "    2*layer_norm +\n",
    "    attn +\n",
    "    attn_proj +\n",
    "    first_fp +\n",
    "    second_fp\n",
    ")\n",
    "\n",
    "total_params = wte_size + wpe_size + transformer\n",
    "print(f\"Total parameters in GPT-2: {total_params/10**6}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f467ed",
   "metadata": {},
   "source": [
    "which is the same as indicated in [GPT2 documentation](https://huggingface.co/openai-community/gpt2#:~:text=This%20is%20the%20smallest%20version%20of%20GPT%2D2%2C%20with%20124M%20parameters.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6200115",
   "metadata": {},
   "source": [
    "## Compute Accounting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9098d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
