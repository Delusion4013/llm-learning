{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd1b339",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook aims to provide notes on attention mechanism in LLMs and implementations of popular attentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1bd951",
   "metadata": {},
   "source": [
    "#### What is attention?\n",
    "\n",
    "Attention is a computing mechanism to **summarize semantic similarities** by performing dot-product on query-key pairs and return a weighted matrix.\n",
    "\n",
    "#### Why we need attention?\n",
    "\n",
    "In tasks like natural language processing and sequence modeling, the input is often a *variable-length sequence*, such as text, audio, or video frames. Traditional RNN methods (like LSTM, GRU) face *vanishing gradients* or *exploding gradients* when processing long sequences. Also, due to the intrinsic limitations of the recurrent structure, the model struggles to flexibly establish direct and controllable dependencies between different positions, resulting loss of detail for long sequence.\n",
    "\n",
    "The introduction of the Attention mechanism is mainly driven by the idea that the model, when processing the current time step (or current word), should be able to *adaptively* \"attend\" to more important parts of the input sequence and ignore less relevant ones. It does so by explicitly computing the relevance (semantic similarity) between positions, using which to assign weights and extract contextual information.\n",
    "\n",
    "In applications like machine translation, text summarization, and reading comprehension, the need for information from different positions in the input sequence varies at each time step. The Attention mechanism allows the model to dynamically assign “attention weights,” so that a word being generated can emphasize semantically relevant parts of the input. Compared to pure RNN/CNN models, this mechanism performs significantly better in capturing long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f200128",
   "metadata": {},
   "source": [
    "#### Computation steps for Scaled dot-product attention\n",
    "\n",
    "This is the version used in [transformer paper](https://www.arxiv.org/abs/1706.03762).\n",
    "\n",
    "1. Get `Q,K,V`\n",
    "    - `Q` stands for query, it could be the hidden representation of input sequence at certain position, depending on the task and the module.\n",
    "    - `K` stands for key, usually vectors generated from *reference sequence* (same=self-attention, different=cross-attention).\n",
    "    - `V` stands for value, corresponding to `K`, the value vector of each position, representing information sending to query.\n",
    "\n",
    "2. Compute attention score\n",
    "\n",
    "Scaled dot-product from transformer $$\\text{score}(Q,K_i)=\\frac{Q\\cdot K_i}{\\sqrt{d_k}}$$\n",
    "\n",
    "The scaling factor $\\frac{1}{\\sqrt{d_k}}$ is used for preventing too large dot-product, mitigating potential gradient issue.\n",
    "\n",
    "3. Compute attention weight\n",
    "\n",
    "Perform a `softmax` on the scores, representing the weights (*extent of attention*) assigned to different position at reference sequence (K). $\\alpha_i= \\text{softmax}(score(Q,K))$\n",
    "\n",
    "4. Sum up\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\sum_i \\alpha_i \\cdot V_i$$\n",
    "\n",
    "This output vector represents the **contextual representation** of a sequence. Simplifing further we can get the paper's representation:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff37da",
   "metadata": {},
   "source": [
    "#### Self-Attention vs. Cross-Attention\n",
    "\n",
    "For self-attention:\n",
    "- Secnario: Used to model the dependencies between different positions **within the same sequence**\n",
    "- Example: In a Transformer Encoder/Decoder, applying Q,K,V on the same sequence is self-attention.\n",
    "- Advantage: allows the network to directly model interactions between **any two positions** in the sequence, without relying on step-by-step propagation like in recurrent structures.\n",
    "\n",
    "For cross-attention:\n",
    "- Secnario: In tasks like translation, decoder need to look up or align with encoder's output sequence. Therefore the decoder's current hidden state is used as `Q`, and the encoder's output is used as `K` and `V`.\n",
    "- Reason: allows the decoder to **look across sequences** to find the most relevant position in the source sentence representation when generating the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea59c3e",
   "metadata": {},
   "source": [
    "#### Main stream attention methods\n",
    "1. **Scaled Dot-Product Attention** \n",
    "    - After applying linear transformations to Q, K, V, compute attention using \"dot product + softmax + weighted sum\".\n",
    "2. **Multi-Head Attention (MHA)**\n",
    "    - Split Q, K, V into `h` subspaces (heads), compute attention separately in each, then concatenate and project again.\n",
    "    - Advantage: Enables the model to learn different types of attention patterns across different subspaces, enhancing model expressiveness. (like kernels in CNN)\n",
    "3. Flash Attention\n",
    "    - Mainly algorithmic implementation optimization (by calculating in blocks), same formula as scaled dot-product attention.\n",
    "4. Sparse Attention\n",
    "    - restricts calculations to a subset of key positions (e.g., local windows, selected global tokens)\n",
    "    - $$\\text{Attention}_\\text{sparse} (Q,K,V) = \\text{softmax}\\left(\\frac{QK^T_\\text{masked}}{\\sqrt{d_k}}\\right)V$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c533606",
   "metadata": {},
   "source": [
    "#### Weight sharing in Transformer\n",
    "\n",
    "In some variants or specific implementations, to reduce the number of parameters, **weight sharing** is applied across **different layers of the Encoder or Decoder**.\n",
    "\n",
    "This is feasible because:\n",
    "\n",
    "- All layers in a Transformer have the **same structure** (self-attention + feedforward network), forming a stack.\n",
    "- Experiments show that such sharing **does not significantly degrade model performance**, while offering the benefit of **greatly reducing parameter count**.\n",
    "\n",
    "However, note that the original Transformer paper (Vaswani et al.) **does not mandate sharing across all layers**. In practice, this can be flexibly chosen based on project needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d796d5",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads,dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize projection matrices for Q, K, V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Project inputs to Q, K, V\n",
    "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf')) # Fill in a large negative value to ensure after softmax it's 0\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Concatenate heads and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        return self.out_proj(attn_output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
